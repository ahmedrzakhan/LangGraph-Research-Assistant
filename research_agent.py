"""
LangGraph Research Assistant
============================
A simple research assistant that demonstrates LangGraph's core concepts:
- State management with TypedDict
- Node functions for processing
- Graph compilation and execution
- Conditional edges for workflow control

This agent takes a research topic and:
1. Plans key questions to research
2. Researches answers to each question
3. Summarizes findings into a final report
"""

import os
from typing import TypedDict, List, Annotated
from dotenv import load_dotenv

# LangGraph imports
from langgraph.graph import StateGraph, START, END

# LangChain imports for LLM interaction with Hugging Face
from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace
from langchain_core.messages import HumanMessage, SystemMessage

# Load environment variables from .env file
load_dotenv()


# =============================================================================
# STATE DEFINITION
# =============================================================================
# TypedDict defines the structure of data that flows through the graph.
# Each node can read from and write to this shared state.

class ResearchState(TypedDict):
    """
    The state object that gets passed between nodes in our graph.

    Attributes:
        topic: The research topic provided by the user
        questions: List of research questions generated by the planner
        answers: List of answers to each research question
        final_report: The summarized research report
        current_step: Tracks which step we're on for progress display
    """
    topic: str
    questions: List[str]
    answers: List[str]
    final_report: str
    current_step: str


# =============================================================================
# LLM SETUP
# =============================================================================
# Initialize the Hugging Face model that will power our research assistant

def get_llm():
    """
    Create and return a ChatHuggingFace instance.
    Uses Qwen2.5-72B-Instruct via Hugging Face Inference API.
    You can change the model to any supported model on Hugging Face Hub.
    """
    # Create the Hugging Face endpoint
    llm = HuggingFaceEndpoint(
        repo_id="Qwen/Qwen2.5-72B-Instruct",  # High-quality, well-supported model
        task="text-generation",
        max_new_tokens=512,
        temperature=0.7,  # Moderate creativity for research tasks
    )
    # Wrap it in ChatHuggingFace for chat-style interactions
    return ChatHuggingFace(llm=llm)


# =============================================================================
# NODE FUNCTIONS
# =============================================================================
# Each node is a function that takes the current state and returns updates to it.
# Nodes represent discrete steps in our workflow.

def planner_node(state: ResearchState) -> dict:
    """
    PLANNER NODE
    ------------
    Takes the research topic and generates 3 key questions to investigate.

    This demonstrates:
    - Reading from state (topic)
    - Using LLM to generate structured output
    - Returning state updates (questions)
    """
    print(f"\nüìã PLANNER: Breaking down topic into key questions...")

    topic = state["topic"]
    llm = get_llm()

    # Create a prompt that asks the LLM to generate research questions
    messages = [
        SystemMessage(content="""You are a research planner. Given a topic,
        generate exactly 3 key questions that would help someone understand
        the topic comprehensively. Return ONLY the questions, one per line,
        numbered 1-3."""),
        HumanMessage(content=f"Topic: {topic}")
    ]

    # Call the LLM
    response = llm.invoke(messages)

    # Parse the response into a list of questions
    questions = [q.strip() for q in response.content.strip().split("\n") if q.strip()]

    # Display the generated questions
    print(f"   Generated {len(questions)} research questions:")
    for q in questions:
        print(f"   ‚Ä¢ {q}")

    # Return state updates - this gets merged with existing state
    return {
        "questions": questions,
        "current_step": "planning_complete"
    }


def researcher_node(state: ResearchState) -> dict:
    """
    RESEARCHER NODE
    ---------------
    Takes each question and generates a research answer.

    This demonstrates:
    - Iterating over state data (questions)
    - Making multiple LLM calls
    - Aggregating results into state (answers)
    """
    print(f"\nüîç RESEARCHER: Investigating each question...")

    questions = state["questions"]
    topic = state["topic"]
    llm = get_llm()

    answers = []

    for i, question in enumerate(questions, 1):
        print(f"   Researching question {i}/{len(questions)}...")

        # Create a prompt for researching each question
        messages = [
            SystemMessage(content=f"""You are a research assistant investigating
            the topic: {topic}. Provide a concise but informative answer
            (2-3 sentences) to the following question. Focus on key facts
            and insights."""),
            HumanMessage(content=question)
        ]

        response = llm.invoke(messages)
        answers.append(response.content.strip())
        print(f"   ‚úì Question {i} answered")

    return {
        "answers": answers,
        "current_step": "research_complete"
    }


def summarizer_node(state: ResearchState) -> dict:
    """
    SUMMARIZER NODE
    ---------------
    Combines all research findings into a cohesive final report.

    This demonstrates:
    - Reading multiple state fields (topic, questions, answers)
    - Synthesizing information
    - Producing final output (final_report)
    """
    print(f"\nüìù SUMMARIZER: Creating final research report...")

    topic = state["topic"]
    questions = state["questions"]
    answers = state["answers"]
    llm = get_llm()

    # Combine questions and answers for context
    qa_pairs = "\n\n".join([
        f"Q: {q}\nA: {a}"
        for q, a in zip(questions, answers)
    ])

    messages = [
        SystemMessage(content="""You are a research summarizer. Given a topic
        and Q&A pairs from research, create a well-structured summary report.
        Include:
        1. A brief introduction
        2. Key findings (bullet points)
        3. A conclusion
        Keep it concise but comprehensive."""),
        HumanMessage(content=f"Topic: {topic}\n\nResearch Findings:\n{qa_pairs}")
    ]

    response = llm.invoke(messages)

    print("   ‚úì Report generated")

    return {
        "final_report": response.content.strip(),
        "current_step": "complete"
    }


# =============================================================================
# CONDITIONAL EDGE FUNCTION
# =============================================================================
# Conditional edges determine which node to visit next based on state

def should_continue(state: ResearchState) -> str:
    """
    Determines the next step in the workflow based on current state.

    This demonstrates conditional routing in LangGraph.
    Returns the name of the next node to visit.
    """
    current_step = state.get("current_step", "")

    if current_step == "planning_complete":
        return "researcher"
    elif current_step == "research_complete":
        return "summarizer"
    else:
        return END


# =============================================================================
# GRAPH CONSTRUCTION
# =============================================================================
# Build the state graph that defines our workflow

def create_research_graph():
    """
    Creates and compiles the research assistant graph.

    Graph Structure:
    START -> planner -> researcher -> summarizer -> END

    Returns:
        CompiledGraph: The compiled graph ready for execution
    """
    # Initialize a StateGraph with our state schema
    workflow = StateGraph(ResearchState)

    # Add nodes to the graph
    # Each node is identified by a string name and associated with a function
    workflow.add_node("planner", planner_node)
    workflow.add_node("researcher", researcher_node)
    workflow.add_node("summarizer", summarizer_node)

    # Add edges to define the flow
    # START -> planner: The entry point of our graph
    workflow.add_edge(START, "planner")

    # Add conditional edges from planner
    # This demonstrates dynamic routing based on state
    workflow.add_conditional_edges(
        "planner",
        should_continue,
        {
            "researcher": "researcher",
            "summarizer": "summarizer",
            END: END
        }
    )

    # Add conditional edges from researcher
    workflow.add_conditional_edges(
        "researcher",
        should_continue,
        {
            "summarizer": "summarizer",
            END: END
        }
    )

    # summarizer -> END: Final node leads to completion
    workflow.add_edge("summarizer", END)

    # Compile the graph - this validates the structure and prepares for execution
    return workflow.compile()


# =============================================================================
# GRAPH VISUALIZATION
# =============================================================================

def visualize_graph(graph):
    """
    Generate a Mermaid diagram of the graph structure.
    Useful for documentation and understanding the workflow.
    """
    print("\nüìä Graph Structure (Mermaid Diagram):")
    print("-" * 40)
    print(graph.get_graph().draw_mermaid())
    print("-" * 40)


# =============================================================================
# MAIN EXECUTION
# =============================================================================

def run_research(topic: str) -> str:
    """
    Execute the research workflow for a given topic.

    Args:
        topic: The research topic to investigate

    Returns:
        str: The final research report
    """
    # Create the graph
    graph = create_research_graph()

    # Initialize the starting state
    initial_state = {
        "topic": topic,
        "questions": [],
        "answers": [],
        "final_report": "",
        "current_step": ""
    }

    print(f"\n{'='*60}")
    print(f"üî¨ RESEARCH ASSISTANT")
    print(f"{'='*60}")
    print(f"Topic: {topic}")
    print(f"{'='*60}")

    # Execute the graph
    # The graph will process through each node, updating state along the way
    final_state = graph.invoke(initial_state)

    return final_state["final_report"]


def main():
    """
    CLI interface for the research assistant.
    """
    print("\n" + "="*60)
    print("       üî¨ LangGraph Research Assistant üî¨")
    print("="*60)

    # Check for API key
    if not os.getenv("HUGGINGFACEHUB_API_TOKEN"):
        print("\n‚ùå Error: HUGGINGFACEHUB_API_TOKEN not found in environment variables.")
        print("   Please copy .env.example to .env and add your Hugging Face token.")
        return

    # Get topic from user
    print("\nEnter a research topic (or 'quit' to exit):")

    while True:
        topic = input("\nüìå Topic: ").strip()

        if topic.lower() in ['quit', 'exit', 'q']:
            print("\nGoodbye! üëã")
            break

        if not topic:
            print("Please enter a valid topic.")
            continue

        try:
            # Run the research
            report = run_research(topic)

            # Display results
            print(f"\n{'='*60}")
            print("üìÑ FINAL RESEARCH REPORT")
            print(f"{'='*60}")
            print(report)
            print(f"{'='*60}")

            # Option to visualize graph
            show_graph = input("\nShow graph structure? (y/n): ").strip().lower()
            if show_graph == 'y':
                graph = create_research_graph()
                visualize_graph(graph)

        except Exception as e:
            print(f"\n‚ùå Error during research: {e}")
            print("   Please check your API key and try again.")

        print("\nEnter another topic or 'quit' to exit:")


if __name__ == "__main__":
    main()
